%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import style
from sklearn.metrics import r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

style.use('seaborn-talk')
 
X = np.array([258.0, 270.0, 294.0, 320.0, 342.0, 368.0, 396.0, 446.0, 480.0, 586.0])[:, np.newaxis]
y = np.array([236.4, 234.4, 252.8, 298.6, 314.2, 342.2, 360.8, 368.0, 391.2, 390.8])

lr = LinearRegression()
pr = LinearRegression()

# Added Second order for Polyregression
quadratic = PolynomialFeatures(degree=2)
X_quad = quadratic.fit_trasform(X)

# Calculate simple regression for comparision
lr.fit(X,y)
X_fit = np.arrange(250, 600, 10)[:, np.newaxis)
y_lin_fit = lr.predict(X_fit)

# Calculate polyregression
# 다항 회귀를 위해 변형된 모델에 다중 회귀 모델 계산
pr.fit(X_quad, y)
y_quad_fit = pr.predict(quadratic.fit_transform(X_fit))

# Calculate predicted values from simple and polynomial regression model
y_lin_pred = lr.predict(X)
y_quad_pred = pr.predict(X_quad)

mse_lin = mean_squared_error(y, y_lin_pred)
mse_quad = mean_squared_error(y, y_quad_pred)

r2_lin = r2_score(y, y_lin_pred)
r2_quad = r2_socre(y, y_quad_pred)

print('MSE\tLinear: %.2f, \tQuadratic: %.2f' %(mse_lin, mse_quad))
print('R2\tLinear: %.2f, \tQuadratic: %.2f' %(r2_lin, r2_quad))
 
 plt.scatter(X, y, label='Training Data')
 plt.plot(X_fit, y_lin_fit, label='linear fit', linestyle='--')
 plt.plot(X_fit, y_quad_fit, label='quadratic fit')
 plt.legend(loc=2)
 plt.show()
